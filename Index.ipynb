{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd68569",
   "metadata": {},
   "source": [
    "### **A Comparative Analysis of Public Sentiment on Twitter towards Apple and Google Products using Natural Language Processing.**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d33c7",
   "metadata": {},
   "source": [
    "**Group Name: Group 5**\n",
    "\n",
    "**Members**\n",
    "1. *Rose Miriti*\n",
    "2. *Isaac Wadhare*\n",
    "3. *Lydia Chumba*\n",
    "4. *Erick Mauti*\n",
    "5. *Marilyn Akinyi*\n",
    "6. *Rodgers Otieno*\n",
    "7. *Samwel Ongechi*\n",
    "\n",
    "**Technical Mentor: George Kamundia**\n",
    "\n",
    "**Phase: Phase 4 Project**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2347084b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets on Apple and Google Products\n",
    "\n",
    "##  Summary\n",
    "\n",
    "This project focuses on analyzing public sentiment expressed on Twitter regarding Apple and Google products, using a labeled dataset of over 9,000 tweets categorized as positive, negative, or neutral. The goal is to build a proof-of-concept NLP model capable of classifying tweets according to sentiment, providing actionable insights that can guide business strategy, marketing, and product development for the two companies.\n",
    "\n",
    "The workflow begins with **business and data understanding**, where the problem is defined, the dataset is explored, and the distribution of sentiment classes is analyzed. The **data preparation** stage includes text cleaning, tokenization, stopword removal, and lemmatization and stemming. Text data will then be transformed into numerical representations using TF-IDF vectors or word embeddings, creating features suitable for machine learning models.\n",
    "\n",
    "For **modeling**, baseline models such as Logistic Regression and Naive Bayes will be implemented first to establish performance benchmarks. For advanced modeling, we will implement a **neural network**. We will also employ Ensemble methods like Random Forest and Gradient Boosting (XGBoost). \n",
    "\n",
    "A **validation strategy** using stratified train-test splits and K-Fold cross-validation will ensure the models generalize well to unseen data. **Evaluation metrics** will include accuracy, precision, recall, F1-score, and confusion matrices to assess multiclass classification performance. The project will produce insights into public sentiment trends, which will directly inform recommendations answering key objectives regarding customer perception, sentiment drivers, and business strategies.\n",
    "\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "Apple and Google face continuous public scrutiny on social media regarding product launches and services. Understanding real-time customer sentiment is critical to improve products, marketing strategies, and customer satisfaction.  \n",
    "\n",
    "**Business problem:**  \n",
    "*\"Can we automatically classify the sentiment of tweets about Apple and Google products to support actionable business insights?\"*\n",
    "\n",
    "\n",
    "## Project Objectives\n",
    "\n",
    "1. **Determine the overall public sentiment** towards Apple and Google products on Twitter.  \n",
    "2. **Identify tweet characteristics and themes** that contribute to positive, negative, or neutral sentiment.  \n",
    "3. **Provide actionable insights** from sentiment trends to inform business decisions, marketing strategies, and product improvements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fc2c8",
   "metadata": {},
   "source": [
    "## 1.0 Importing the necessary libraries for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e45cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import NLTK specific modules\n",
    "from nltk.corpus import stopwords, gutenberg\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import FreqDist, bigrams, trigrams, ngrams\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk import CFG, ChartParser\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# Import scikit-learn for machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Environment setup complete!\")\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ff8b2",
   "metadata": {},
   "source": [
    "## 1.1 Loading the dataset\n",
    "- Loading the data into a pandas DataFrame and view the first 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d3ed6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tweet_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "emotion_in_tweet_is_directed_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "is_there_an_emotion_directed_at_a_brand_or_product",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "cc81e731-28f6-40f5-89c2-93729280ef3b",
       "rows": [
        [
         "0",
         ".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.",
         "iPhone",
         "Negative emotion"
        ],
        [
         "1",
         "@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW",
         "iPad or iPhone App",
         "Positive emotion"
        ],
        [
         "2",
         "@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.",
         "iPad",
         "Positive emotion"
        ],
        [
         "3",
         "@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw",
         "iPad or iPhone App",
         "Negative emotion"
        ],
        [
         "4",
         "@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)",
         "Google",
         "Positive emotion"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data\\judge-1377884607_tweet_product_company.csv', encoding = 'latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169a16d",
   "metadata": {},
   "source": [
    "## 1.2 Data Inspecting \n",
    "- Data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bdfac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Structure\n",
      "Shape: (9093, 3) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n",
      "None \n",
      "\n",
      "OBSERVATION:\n",
      "There are 3 features. \n",
      "And 9093 records in our dataset.\n",
      "All features are categorical.\n",
      "There are missing values in two columns: 'tweet_text' and 'emotion_in_tweet_is_directed_at'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Structure\")\n",
    "print(f\"Shape: {df.shape} \\n\")\n",
    "print(f'{df.info()} \\n')  \n",
    "print(\"OBSERVATION:\")\n",
    "print(f\"There are {df.shape[1]} features. \" )\n",
    "print(f\"And {df.shape[0]} records in our dataset.\" )\n",
    "print(\"All features are categorical.\")\n",
    "print(\"There are missing values in two columns: 'tweet_text' and 'emotion_in_tweet_is_directed_at'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e484559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_text                                             0.010997\n",
      "emotion_in_tweet_is_directed_at                       63.807324\n",
      "is_there_an_emotion_directed_at_a_brand_or_product     0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((df.isnull().sum()/len(df))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e0e09",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- **Missing Values**:  \n",
    "  - `tweet_text`: 0.01%  \n",
    "  - `emotion_in_tweet_is_directed_at`: 63.8% missing values.  \n",
    "  - Target column has **no missing values**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adfde86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22 duplicates in our dataset. \n",
      "We need to drop them to prevent false outcome .\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates\n",
    "print(f\"There are {df.duplicated().sum()} duplicates in our dataset. \\nWe need to drop them to prevent false outcome .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50acf06",
   "metadata": {},
   "source": [
    "- Check uniques values for sentiment features and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80eed001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_there_an_emotion_directed_at_a_brand_or_product\n",
      "No emotion toward brand or product    5389\n",
      "Positive emotion                      2978\n",
      "Negative emotion                       570\n",
      "I can't tell                           156\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Observation:\n",
      "There is need to merge neutral sentiments and remove the word 'emotion' from positive and negative sentiments\n",
      "Neutral emotions holds more than 50% of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()} \\n\")\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"There is need to merge neutral sentiments and remove the word 'emotion' from positive and negative sentiments\")\n",
    "print(\"Neutral emotions holds more than 50% of the dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c9a39",
   "metadata": {},
   "source": [
    "## 2.0 Data Cleaning\n",
    "**Involves 3 stages:**\n",
    "\n",
    "### 2.1 Stage 1:\n",
    "Involves:\n",
    "- **Column Standardization** - Shortening names for easy reference \n",
    "- **Removing duplicates**- Preventing duplicate samples from overweighting certain classes hence every unique tweet target pair appears only once.\n",
    "- **Mapping sentiment unique values**- Consolidates the neutral emotions and remove emotions from the other parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c3a0a",
   "metadata": {},
   "source": [
    "- For this project, we simplify the dataset into three main sentiment classes: \n",
    "**positive, negative, and neutral.**  \n",
    "- **\"Positive emotion\": Positive**  \n",
    "- **\"Negative emotion\" : Negative**  \n",
    "- **\"I can't tell\" : Neutral**  \n",
    "  This reflects uncertainty. It is not clear whether the sentiment is  positive or \n",
    "  negative. Mapping it to *neutral* ensures \n",
    "  the model doesn’t misinterpret it as an opinion.\n",
    "\n",
    "- **\"No emotion toward brand or product\": Neutral**  \n",
    "  This explicitly signals the absence of any emotional reaction hence it fits best under *neutral*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299d1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Standardization\n",
    "df = df.rename(columns= {'tweet_text':'text','emotion_in_tweet_is_directed_at':'brand',\n",
    "                         'is_there_an_emotion_directed_at_a_brand_or_product':'sentiment'})\n",
    "\n",
    "\n",
    "#Removing duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#Mapping sentiment unique values\n",
    "sentiment_mapping = {\n",
    "    'Negative emotion': 'Negative', \n",
    "    'Positive emotion':'Positive',\n",
    "    'No emotion toward brand or product': 'Neutral',\n",
    "    \"I can't tell\":'Neutral'\n",
    "}\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca33dda8",
   "metadata": {},
   "source": [
    "### 2.2 Stage 2:\n",
    "- **Handling missing values**\n",
    "- Use a dictionary to populate either Apple or Google in all instances where the tweet contains the product names in the list<br>\n",
    "Index will be used as keys in the dictionary for the sake of matching it with the tweet index later for appropriate categorization<br>\n",
    "- Replacing null on the brand column with 'Unknown' for the remaining dataset. And dropping null values on the 'text' column the remaining null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d3357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand distribution BEFORE imputation:\n",
      "brand\n",
      "NaN                                5788\n",
      "iPad                                945\n",
      "Apple                               659\n",
      "iPad or iPhone App                  469\n",
      "Google                              428\n",
      "iPhone                              296\n",
      "Other Google product or service     293\n",
      "Android App                          80\n",
      "Android                              77\n",
      "Other Apple product or service       35\n",
      "Name: count, dtype: int64\n",
      "Missing brands: 5788\n",
      "\n",
      "Brand distribution AFTER imputation:\n",
      "brand\n",
      "Apple                              3834\n",
      "Google                             2309\n",
      "iPad                                945\n",
      "Unknown                             732\n",
      "iPad or iPhone App                  469\n",
      "iPhone                              296\n",
      "Other Google product or service     293\n",
      "Android App                          80\n",
      "Android                              77\n",
      "Other Apple product or service       35\n",
      "Name: count, dtype: int64\n",
      "Missing brands: 0\n",
      "\n",
      "Newly imputed brands breakdown:\n",
      "brand\n",
      "Apple      3175\n",
      "Google     1881\n",
      "Unknown     732\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of newly imputed texts:\n",
      "Text: @teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conferen...\n",
      "→ Imputed Brand: Apple\n",
      "---\n",
      "Text: Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw\n",
      "→ Imputed Brand: Apple\n",
      "---\n",
      "Text: Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}\n",
      "→ Imputed Brand: Google\n",
      "---\n",
      "Text: Anyone at  #sxsw want to sell their old iPad?\n",
      "→ Imputed Brand: Apple\n",
      "---\n",
      "Text: Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?\n",
      "→ Imputed Brand: Apple\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Dropping null for text column\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Your brand lists (converted to lowercase for case-insensitive matching)\n",
    "google_products = [product.lower() for product in [\n",
    "    \"google\", \"gmail\", \"gdrive\", \"google drive\", \"gdocs\", \"google docs\",\n",
    "    \"gsheets\", \"google sheets\", \"gslides\", \"google slides\", \"gmeet\", \"google meet\",\n",
    "    \"google calendar\", \"chrome\", \"chromebook\", \"chromecast\", \"chromium\",\n",
    "    \"pixel\", \"pixelbook\", \"nest\", \"google home\", \"android\", \"wear os\", \"play store\",\n",
    "    \"google maps\", \"google earth\", \"waze\", \"google photos\", \"youtube\", \"yt music\",\n",
    "    \"google cloud\", \"gcp\", \"google workspace\", \"google ads\", \"adsense\", \"doubleclick\"\n",
    "]]\n",
    "\n",
    "apple_products = [product.lower() for product in [\n",
    "    \"apple\", \"icloud\", \"itunes\", \"apple music\", \"apple tv\", \"apple tv\", \"apple arcade\",\n",
    "    \"apple pay\", \"apple news\", \"apple podcasts\", \"apple watch\", \"watchos\",\n",
    "    \"iphone\", \"ipad\", \"ipados\", \"imac\", \"macbook\", \"macbook air\", \"macbook pro\",\n",
    "    \"mac mini\", \"mac pro\", \"airpods\", \"airpods pro\", \"airpods max\",\n",
    "    \"safari\", \"siri\", \"facetime\", \"imessage\", \"keynote\", \"numbers\", \"pages\",\n",
    "    \"app store\", \"ios\", \"macos\", \"homepod\", \"find my\", \"testflight\", \"xcode\", \"iwatch\", \"mac\",\"macpro\"\n",
    "]]\n",
    "\n",
    "# Function to identify brand from text\n",
    "def identify_brand_from_text(text):\n",
    "    \"\"\"\n",
    "    Identify if text contains words related to Google or Apple products.\n",
    "    Returns 'Apple', 'Google', or 'Unknown'.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Check for Apple products\n",
    "    for product in apple_products:\n",
    "        if re.search(r'\\b' + re.escape(product) + r'\\b', text_lower):\n",
    "            return 'Apple'\n",
    "    \n",
    "    # Check for Google products\n",
    "    for product in google_products:\n",
    "        if re.search(r'\\b' + re.escape(product) + r'\\b', text_lower):\n",
    "            return 'Google'\n",
    "    \n",
    "    return 'Unknown'\n",
    "\n",
    "# Check current brand distribution before imputation\n",
    "print(\"Brand distribution BEFORE imputation:\")\n",
    "print(df['brand'].value_counts(dropna=False))\n",
    "print(f\"Missing brands: {df['brand'].isna().sum()}\")\n",
    "\n",
    "# Only impute brands for rows where brand is missing\n",
    "mask = df['brand'].isna()\n",
    "df.loc[mask, 'brand'] = df.loc[mask, 'text'].apply(identify_brand_from_text)\n",
    "\n",
    "# Check brand distribution after imputation\n",
    "print(\"\\nBrand distribution AFTER imputation:\")\n",
    "print(df['brand'].value_counts(dropna=False))\n",
    "print(f\"Missing brands: {df['brand'].isna().sum()}\")\n",
    "\n",
    "# Check what was imputed\n",
    "newly_imputed = df[df['brand'].notna() & mask].copy()\n",
    "print(f\"\\nNewly imputed brands breakdown:\")\n",
    "print(newly_imputed['brand'].value_counts())\n",
    "\n",
    "# Show some examples of newly imputed brands\n",
    "print(\"\\nSample of newly imputed texts:\")\n",
    "for i, row in newly_imputed.head(5).iterrows():\n",
    "    text_preview = str(row['text'])[:100] + \"...\" if len(str(row['text'])) > 100 else str(row['text'])\n",
    "    print(f\"Text: {text_preview}\")\n",
    "    print(f\"→ Imputed Brand: {row['brand']}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
